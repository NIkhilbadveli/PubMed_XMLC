{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Important Note:- This is the main notebook file but there are other supplementary code and data files for this notebook to work properly. They are moved into the folder \"Supplementary files\". Please copy them into this folder before proceeding further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 1. Econbiz dataset\n",
    "\n",
    "The [EconBiz dataset](https://www.kaggle.com/datasets/hsrobo/titlebased-semantic-subject-indexing) was compiled from a meta-data export provided by ZBW - Leibniz Information Centre for Economics from July 2017. The annotations were selected by human annotators from the Standard Thesaurus Wirtschaft (STW), which contains approximately 5,700 labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv('econbiz.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Check the data types of the columns\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Describe the data\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get maximum length of document in the 'title' column\n",
    "df.title.str.len().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Plot histogram of document length in the 'title' column\n",
    "df.title.str.len().hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Plot histogram of number of labels in the 'labels' column\n",
    "df.labels.str.split('\\t').str.len().hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get vocabulary size of the 'title' column by doing TfIdfVectorizer on the 'title' column\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(df.title)\n",
    "len(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 2. Pubmed dataset\n",
    "\n",
    "The [PubMed dataset](https://www.kaggle.com/datasets/hsrobo/titlebased-semantic-subject-indexing) was compiled from the training set of the 5th BioASQ challenge on large-scale semantic subject indexing of biomedical articles, which were all in English. Again, we removed duplicates by checking for same title and labels. In total, approximately 12.8 million publications remain.\n",
    "The labels are so called MeSH terms. In our data, approximately 28k of them are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv('pubmed.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Check the data types of the columns\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Describe the data\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get maximum length of document in the 'title' column\n",
    "df.title.str.len().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Plot histogram of document length in the 'title' column\n",
    "df.title.str.len().hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Plot histogram of number of labels in the 'labels' column\n",
    "df.labels.str.split('\\t').str.len().hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get vocabulary size of the 'title' column by doing TfIdfVectorizer on the 'title' column\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(df.title)\n",
    "len(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Using Omikuji (RUST implementation of Parabel) for eXtreme Multi-label Classification (XMLC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Here are the instructions to install [Rust](https://doc.rust-lang.org/cargo/getting-started/installation.html) and [Omikuji](https://github.com/tomtung/omikuji) that were followed in the below steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 1. Prepare data and install Rust, Omikuji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from process_data import create_parabel_data_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define constants\n",
    "DATASET = 'econbiz'\n",
    "RAW_DATA = DATASET + '/econbiz.csv'\n",
    "RESULTS_DIR = DATASET + '/Results'\n",
    "MODEL_DIR = DATASET + '/Model'\n",
    "\n",
    "PRED_FILE = RESULTS_DIR + '/{}_pred.txt'.format(DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create train and test .txt data files according to the Parabel data format mentioned here (https://github.com/tomtung/omikuji#data-format). \n",
    "train_fname, test_fname = create_parabel_data_files(dataset=DATASET, raw_data_file=RAW_DATA)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Install Omikuji using Cargo that should've been installed before this step.\n",
    "!cargo install omikuji --features cli --locked"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set this variable to see full backtrace of the error\n",
    "% env RUST_BACKTRACE=full"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check help to see command options for Omikuji\n",
    "!omikuji train --help"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2. Train the model\n",
    "\n",
    "Don't forget to clear the model directory before training. Otherwise, you'll face an error."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train the model by specifying the train data file path and the model path\n",
    "!omikuji train $train_fname --model_path $MODEL_DIR"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3. Evaluate the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Finally evaluate the model on the \n",
    "!omikuji test $MODEL_DIR $test_fname --out_path $PRED_FILE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As can be seen at the end of the testing, the precision with default parameters is Precision@[1, 3, 5] = [73.78, 54.11, 41.08]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**This is the end of the code that is being submitted as part of the assignment.**\n",
    "\n",
    "Below code are the multiple experiments that were tried during the course of this assignment. I've tried doing the BaseMLP model mentioned in the paper referenced in the assignment. Some compatability issues with Keras package have been raised.\n",
    "\n",
    "Similarily, with Parabel paper where the authors have generously provided the complete source code and the binaries to test their algorithm. Since the code is in C++ and some Matlab scripts were present I installed both but once again due to data format issues, couldn't continue further."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As can be seen at the end of the testing, the precision with default parameters is Precision@[1, 3, 5] = [73.78, 54.11, 41.08]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**This is the end of the code that is being submitted as part of the assignment.**\n",
    "\n",
    "Below code are the multiple experiments that were tried during the course of this assignment. I've tried doing the BaseMLP model mentioned in the paper referenced in the assignment. Some compatability issues with Keras package have been raised.\n",
    "\n",
    "Similarily, with Parabel paper where the authors have generously provided the complete source code and the binaries to test their algorithm. Since the code is in C++ and some Matlab scripts were present I installed both but once again due to data format issues, couldn't continue further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Using MLP for eXtreme Multi-label Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 1. Test the template code shared by one of the paper's author on Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "SINGLE_FOLD = True\n",
    "ALL_TITLES = False\n",
    "RAW_CSV_FILE = \"econbiz/econbiz.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_dataset(dataset_path, fold_i, all_titles=False):\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    if not all_titles:\n",
    "        df = df[df[\"fold\"].isin(range(0, 10))]\n",
    "\n",
    "    labels = df[\"labels\"].values\n",
    "    labels = [[l for l in label_string.split()] for label_string in labels]\n",
    "    multilabel_binarizer = MultiLabelBinarizer(sparse_output=True)\n",
    "    multilabel_binarizer.fit(labels)\n",
    "\n",
    "    def to_indicator_matrix(some_df):\n",
    "        some_df_labels = some_df[\"labels\"].values\n",
    "        some_df_labels = [[l for l in label_string.split()] for label_string in some_df_labels]\n",
    "        return multilabel_binarizer.transform(some_df_labels)\n",
    "\n",
    "    test_df = df[df[\"fold\"] == fold_i]\n",
    "    X_test = test_df[\"title\"].values\n",
    "    y_test = to_indicator_matrix(test_df)\n",
    "\n",
    "    train_df = df[df[\"fold\"] != fold_i]\n",
    "    X_train = train_df[\"title\"].values\n",
    "    y_train = to_indicator_matrix(train_df)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# for demonstration, employ TFIDF with binary relevance logistic regression\n",
    "clf = Pipeline(\n",
    "    [(\"vectorizer\", TfidfVectorizer(max_features=25000)),\n",
    "     (\"classifier\", OneVsRestClassifier(LogisticRegression(), n_jobs=4))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(dataset):\n",
    "    scores = []\n",
    "    for i in range(0, 10):\n",
    "        train_df, y_train, test_df, y_test = load_dataset(dataset, i, all_titles=ALL_TITLES)\n",
    "        print('Shapes of X_train, y_train, X_test, y_test', train_df.shape, y_train.shape, test_df.shape, y_test.shape)\n",
    "        clf.fit(train_df, y_train)\n",
    "        y_pred = clf.predict(test_df)\n",
    "\n",
    "        scores.append(f1_score(y_test, y_pred, average=\"samples\"))\n",
    "\n",
    "        if SINGLE_FOLD:\n",
    "            break\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"EconBiz average F-1 score:\", evaluate(RAW_CSV_FILE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 2. Implement the BaseMLP model mentioned in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'mlp_for_xmlc' from 'C:\\\\Users\\\\nikhi\\\\PycharmProjects\\\\Assignments\\\\CE807\\\\Assignment2\\\\mlp_for_xmlc.py'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlp_for_xmlc as mlx\n",
    "import importlib\n",
    "\n",
    "importlib.reload(mlx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x_train, Y_train, x_test, Y_test = load_dataset(RAW_CSV_FILE, fold_i=0, all_titles=ALL_TITLES)\n",
    "print('Shapes of X_train, y_train, X_test, y_test', x_train.shape, Y_train.shape, x_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mlp = mlx.MLP(verbose=1)\n",
    "tp = mlx.ThresholdingPredictor(mlp, alpha=1.0, stepsize=0.01, verbose=1)\n",
    "tp.fit(x_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = tp.predict(x_test)\n",
    "print(\"Mean F1 score:\", f1_score(Y_test, y_pred, average=\"samples\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Using Parabel paper code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 1. Prepare data in the format required by the Parabel binaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset = 'econbiz'\n",
    "data_dir = dataset + '/Data'\n",
    "results_dir = dataset + '/Results'\n",
    "model_dir = dataset + '/Model'\n",
    "\n",
    "trn_ft_file = data_dir + '/trn_X_Xf.txt'\n",
    "trn_lbl_file = data_dir + '/trn_X_Y.txt'\n",
    "tst_ft_file = data_dir + '/tst_X_Xf.txt'\n",
    "tst_lbl_file = data_dir + '/tst_X_Y.txt'\n",
    "score_file = results_dir + '/score_mat.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from process_data import create_parabel_data_files_v1\n",
    "\n",
    "create_parabel_data_files_v1(dataset, RAW_CSV_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# training\n",
    "# Reads training features (in %trn_ft_file%), training labels (in %trn_lbl_file%), and writes Parabel model to %model_dir%\n",
    "!parabel_train $trn_ft_file $trn_lbl_file $model_dir -T 1 -s 0 -t 3 -b 1.0 -c 1.0 -m 100 -tcl 0.1 -tce 0 -e 0.0001 -n 20 -k 0 -q 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# testing\n",
    "# Reads test features (in %tst_ft_file%), FastXML model (in %model_dir%), and writes test label scores to %score_file%\n",
    "!parabel_predict $tst_ft_file $model_dir $score_file -t 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 2. Evaluate the performance of the model.\n",
    "\n",
    "##### Example based\n",
    "The metrics are computed in a per datapoint manner. For each predicted label its only its score is computed, and then these scores are aggregated over all the datapoints.\n",
    "\n",
    "Precision = 1n∑ni=1|Yi∩h(xi)||h(xi)| , The ratio of how much of the predicted is correct. The numerator finds how many labels in the predicted vector has common with the ground truth, and the ratio computes, how many of the predicted true labels are actually in the ground truth.\n",
    "Recall = 1n∑ni=1|Yi∩h(xi)||Yi| , The ratio of how many of the actual labels were predicted. The numerator finds how many labels in the predicted vector has common with the ground truth (as above), then finds the ratio to the number of actual labels, therefore getting what fraction of the actual labels were predicted.\n",
    "There are other metrics as well.\n",
    "\n",
    "##### Label based\n",
    "Here the things are done labels-wise. For each label the metrics (eg. precision, recall) are computed and then these label-wise metrics are aggregated. Hence, in this case you end up computing the precision/recall for each label over the entire dataset, as you do for a binary classification (as each label has a binary assignment), then aggregate it.\n",
    "\n",
    "The easy way is to present the general form.\n",
    "\n",
    "This is just an extension of the standard multi-class equivalent.\n",
    "\n",
    "Macro averaged 1q∑qj=1B(TPj,FPj,TNj,FNj)\n",
    "Micro averaged B(∑qj=1TPj,∑qj=1FPj,∑qj=1TNj,∑qj=1FNj)\n",
    "Here the TPj,FPj,TNj,FNj are the true positive, false positive, true negative and false negative counts respectively for only the jth label.\n",
    "\n",
    "Here B stands for any of the confusion-matrix based metric. In your case you would plug in the standard precision and recall formulas. For macro average you pass in the per label count and then sum, for micro average you average the counts first, then apply your metric function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_labels_from_txt_file(txt_file, prob_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Reads the labels from a txt file.\n",
    "    :param prob_threshold:\n",
    "    :param txt_file:\n",
    "    :return: list of lists of labels for each datapoint\n",
    "    \"\"\"\n",
    "    assert 1.0 >= prob_threshold >= 0.0, \"prob_threshold must be between 0 and 1\"\n",
    "    with open(txt_file) as f:\n",
    "        lines = f.readlines()\n",
    "    labels = []\n",
    "    i = 0\n",
    "    # max_prob = 0\n",
    "    for line in lines[1:]:  # skip the first line\n",
    "        line = line.strip()\n",
    "        # split on space, gives a list of strings each in the format of label:score\n",
    "        lbls_and_scores = line.split()\n",
    "\n",
    "        # split on : and take the first part, which is the label\n",
    "        sub_labels = [x.split(':')[0] for x in lbls_and_scores if float(x.split(':')[1]) >= prob_threshold]\n",
    "\n",
    "        # sub_label_scores = [float(x.split(':')[1]) for x in lbls_and_scores]\n",
    "        # if len(sub_label_scores) > 0:\n",
    "        #     max_prob = max(max_prob, max(sub_label_scores))\n",
    "\n",
    "        labels.append(sub_labels)\n",
    "\n",
    "        if len(sub_labels) == 0 and len(line) > 0:\n",
    "            i += 1\n",
    "            # print(\"Empty label list for line:\", line, i, \"in file:\", txt_file)\n",
    "    print('Number of datapoints with zero predicted labels:', i, 'for file:', txt_file)\n",
    "    # print('Max prob:', max_prob)\n",
    "    return labels\n",
    "\n",
    "\n",
    "def calculate_performance_metrics(lbl_pred, lbl_true):\n",
    "    \"\"\"\n",
    "    Calculates precision, recall and f1-score using the example-based method.\n",
    "    :param lbl_pred:\n",
    "    :param lbl_true:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    assert len(lbl_pred) == len(lbl_true)\n",
    "\n",
    "    ratios = np.array(\n",
    "        [(len(set(h) & set(y)) / len(h), len(set(h) & set(y)) / len(y)) for h, y in zip(lbl_pred, lbl_true) if\n",
    "         len(h) > 0 and len(y) > 0])  # if either of the labels is empty, then the ratio is 0\n",
    "    precision = np.mean(ratios[:, 0])\n",
    "    recall = np.mean(ratios[:, 1])\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    print('Precision:', round(precision * 100, 2), '%')\n",
    "    print('Recall:', round(recall * 100, 2), '%')\n",
    "    print('F1-score:', round(f1 * 100, 2), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = get_labels_from_txt_file(score_file, prob_threshold=0.1)\n",
    "y_true = get_labels_from_txt_file(tst_lbl_file)\n",
    "calculate_performance_metrics(y_pred, y_true)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}